{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "french_all",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/multilingial/blob/master/french_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQqlrXIJej1l",
        "outputId": "8d7b842b-e7c3-482a-e26d-5346795109d8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV04VUZxKYQH",
        "outputId": "ec62f7c1-200c-4caf-c2dd-ccd981a3cbec"
      },
      "source": [
        "pip install sentencepiece "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WXDyhihenRg",
        "outputId": "be74ffdc-11b8-456c-d1e4-a1b8facba767"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ_0F8Zfep7F",
        "outputId": "6201441b-8a46-487a-d38e-a46b90a1055b"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5116  100  5116    0     0  17520      0 --:--:-- --:--:-- --:--:-- 17460\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-nightly ...\n",
            "Done updating TPU runtime\n",
            "Uninstalling torch-1.8.0a0+unknown:\n",
            "  Successfully uninstalled torch-1.8.0a0+unknown\n",
            "Uninstalling torchvision-0.9.0a0+7dfd8dc:\n",
            "  Successfully uninstalled torchvision-0.9.0a0+7dfd8dc\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][123.1 MiB/123.1 MiB]                                                \n",
            "Operation completed over 1 objects/123.1 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][132.0 MiB/132.0 MiB]                                                \n",
            "Operation completed over 1 objects/132.0 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  4.9 MiB/  4.9 MiB]                                                \n",
            "Operation completed over 1 objects/4.9 MiB.                                      \n",
            "Processing ./torch-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (0.8)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.8.0a0+unknown\n",
            "Processing ./torch_xla-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "  Found existing installation: torch-xla 1.6+774f493\n",
            "    Uninstalling torch-xla-1.6+774f493:\n",
            "      Successfully uninstalled torch-xla-1.6+774f493\n",
            "Successfully installed torch-xla-1.6+774f493\n",
            "Processing ./torchvision-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.8.0a0+unknown)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (0.8)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.9.0a0+7dfd8dc\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp5 is already the newest version (5.0.1-1).\n",
            "libopenblas-dev is already the newest version (0.2.20+ds-4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG"
      },
      "source": [
        "import gc\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import date\n",
        "from transformers import *\n",
        "from sklearn.metrics import *\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.optim import *\n",
        "from torch.nn.modules.loss import *\n",
        "from torch.optim.lr_scheduler import * \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "def regular_encode(texts, tokenizer, maxlen=192):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, transformer, num_classes=1):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \n",
        "        Arguments:\n",
        "            model {string} -- Transformer to build the model on. Expects \"camembert-base\".\n",
        "            num_classes {int} -- Number of classes (default: {1})\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        self.transformer = transformer\n",
        "\n",
        "        self.nb_features = self.transformer.pooler.dense.out_features\n",
        "        # for param in self.transformer.parameters():\n",
        "        #   param.requires_grad = False\n",
        "        self.pooler = nn.Sequential(\n",
        "            nn.Linear(self.nb_features, num_classes), \n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        \"\"\"\n",
        "        Usual torch forward function\n",
        "        \n",
        "        Arguments:\n",
        "            tokens {torch tensor} -- Sentence tokens\n",
        "        \n",
        "        Returns:\n",
        "            torch tensor -- Class logits\n",
        "        \"\"\"\n",
        "        hidden_states = self.transformer(\n",
        "            tokens, attention_mask=(tokens > 0).long()\n",
        "        )[1]\n",
        "\n",
        "        # hidden_states = hidden_states[-1][:, 0] # Use the representation of the first token of the last layer\n",
        "\n",
        "        ft = self.pooler(hidden_states)\n",
        "\n",
        "        return ft\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4"
      },
      "source": [
        "class bce(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(bce, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        one=(1-targets)*torch.log(1-inputs)\n",
        "        zero=(targets*torch.log(inputs))\n",
        "        loss = torch.mean((one+zero)*-1)\n",
        "        \n",
        "        return loss\n",
        "class JigsawDataset:\n",
        "    \"\"\"\n",
        "    Torch dataset for training and validating\n",
        "    \"\"\"\n",
        "    def __init__(self, x,y,is_test):\n",
        "        super().__init__()\n",
        "        self.y = y \n",
        "        self.is_test=is_test\n",
        "        self.sentences = x\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sentences.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      len=self.__len__()\n",
        "      if idx>len:\n",
        "        idx=idx%len\n",
        "      if self.is_test==0:\n",
        "        return torch.tensor(self.sentences[idx]), torch.tensor(self.y[idx]).float()\n",
        "      else:\n",
        "        return torch.tensor(self.sentences[idx])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from statistics import mean\n",
        "import torch_xla\n",
        "from sklearn.preprocessing import *\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import scipy as sp\n",
        "import gc\n",
        "import os\n",
        "import cv2\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import random\n",
        "import argparse\n",
        "import sys\n",
        "from statistics import mean\n",
        "import yaml\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import albumentations as A\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import random\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "from tempfile import gettempdir\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def train_all(train_loader, model, device, optimizer):\n",
        "    model.train()\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\n",
        "    model.train()\n",
        "    lss=bce()\n",
        "    loss1=[]\n",
        "    for step, (x, y_batch) in tqdm(enumerate(train_loader),position=0,leave=True): \n",
        "            \n",
        "            # x = x.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(x)\n",
        "            \n",
        "            loss = lss(y_pred.view(-1).float(), y_batch.float())\n",
        "            loss.backward()\n",
        "            loss1.append(loss.item())\n",
        "            xm.optimizer_step(optimizer)\n",
        "            \n",
        "            model.zero_grad()\n",
        "    return mean(loss1)\n",
        "\n",
        "def valid_all(train_loader, model, device):\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\n",
        "    lss=bce()\n",
        "    loss1=[]\n",
        "    for step, (x, y_batch) in enumerate(train_loader): \n",
        "            \n",
        "            # x = x.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(x)\n",
        "            \n",
        "            loss = lss(y_pred.view(-1).float(), y_batch.float())\n",
        "            loss1.append(loss.item())\n",
        "            \n",
        "    return mean(loss1)\n",
        "\n",
        "def predict_all(train_loader, model,device):\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\n",
        "    predict=[]\n",
        "    for step, (x) in tqdm(enumerate(train_loader),position=0,leave=True): \n",
        "            \n",
        "            y_pred = model(x.to(device))\n",
        "            predict.append(y_pred)\n",
        "            \n",
        "    return predict\n",
        "\n",
        "\n",
        "def load_data(lang):\n",
        "    with zipfile.ZipFile('/content/gdrive/My Drive/multilingual/jigsaw-toxic-comment-train-google-'+lang+'-cleaned.csv.zip', 'r') as zip_ref:\n",
        "      zip_ref.extractall('')\n",
        "    trn=pd.read_csv('/content/jigsaw-toxic-comment-train-google-'+lang+'-cleaned.csv',usecols=['toxic','comment_text'])\n",
        "    trn['lang']=lang\n",
        "    tst=pd.read_csv('/content/gdrive/My Drive/multilingual/test.csv.zip',usecols=['lang','content'])  \n",
        "    sub=pd.read_csv('/content/gdrive/My Drive/multilingual/submission.csv')\n",
        "    val=pd.read_csv( '/content/gdrive/My Drive/multilingual/validation.csv.zip',usecols=['lang','comment_text','toxic'])  \n",
        "    tst.columns=['comment_text','lang']\n",
        "    tst['toxic']=sub['toxic']\n",
        "    df=pd.concat([trn,tst,val],0)\n",
        "    return df.loc[df['lang']==lang].reset_index(drop=True).drop(['lang'],1)\n",
        "\n",
        "\n",
        "def get_lang(val,tst,lang):\n",
        "  df=pd.concat([val,tst],0)\n",
        "  return df.loc[df['lang']==lang].reset_index(drop=True).drop(['id','lang'],1)\n",
        "\n",
        "def main():\n",
        "    l1='fr'\n",
        "    lang='french'\n",
        "    link_dk={'fr':'camembert-base',\n",
        "             'pt':'neuralmind/bert-base-portuguese-cased',\n",
        "             'ru':'DeepPavlov/rubert-base-cased',\n",
        "             'tr':'dbmdz/bert-base-turkish-cased',\n",
        "             'es':'dccuchile/bert-base-spanish-wwm-cased',\n",
        "             'it':'dbmdz/bert-base-italian-xxl-cased'\n",
        "             }\n",
        "    epochs=1\n",
        "    batch_size=16\n",
        "    learning_rate=1e-5\n",
        "    seed=42\n",
        "\n",
        "    # Setting seed\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    df=load_data(l1)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(link_dk[l1])\n",
        "    x_train = regular_encode(list(df.comment_text.values), tokenizer, maxlen=192)\n",
        "    y_train = df.toxic.values\n",
        "\n",
        "    idx=df.loc[(df['toxic']>0) & (df['toxic']<1)].index\n",
        "    test=x_train[idx]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def run():\n",
        "\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        device = xm.xla_device()\n",
        "        model = AutoModel.from_pretrained(link_dk[l1])\n",
        "        model=Transformer(model).to(device)\n",
        "\n",
        "\n",
        "        #Training\n",
        "        train_dataset = JigsawDataset(trn_x,trn_y,0)\n",
        "\n",
        "\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            train_dataset,\n",
        "            num_replicas=xm.xrt_world_size(),\n",
        "            rank=xm.get_ordinal(),\n",
        "            shuffle=False\n",
        "        )\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,\n",
        "            sampler=train_sampler,\n",
        "            drop_last=False,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate * xm.xrt_world_size(),weight_decay=1e-3)\n",
        "\n",
        "\n",
        "        xm.master_print(\"Training is Starting ...... \")\n",
        "        total_loss=[]\n",
        "        valid_loss=[]\n",
        "        predictions=[]\n",
        "        for i in tqdm(range(3)):\n",
        "          para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "          total_loss.append(train_all(para_loader.per_device_loader(device),model,device,optimizer))\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        state = { 'state_dict': model.state_dict(),\n",
        "             'optimizer': optimizer.state_dict()}\n",
        "        xm.save(state, lang+str(number))\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "    def _mp_fn(rank, flags):\n",
        "        torch.set_default_tensor_type('torch.FloatTensor')\n",
        "        run()\n",
        "    kf=KFold(n_splits=5, random_state=42, shuffle=False)\n",
        "    number=0\n",
        "    for train_index, test_index in kf.split(range(df.shape[0])):\n",
        "      trn_x=x_train[train_index]\n",
        "      trn_y=y_train[train_index]\n",
        "      number+=1\n",
        "      FLAGS={}\n",
        "      xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES4W36q1Kz7Y",
        "outputId": "9455a73c-546e-443b-e670-02d588c7e282"
      },
      "source": [
        "main()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training is Starting ...... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1465it [13:04,  1.87it/s]\n",
            "\n",
            "1465it [13:04,  1.87it/s]\n",
            "\n",
            "1465it [13:00,  1.88it/s]\n",
            "\n",
            "1465it [13:04,  1.87it/s]\n",
            "\n",
            "1465it [07:53,  3.09it/s]\n",
            "\n",
            " 67%|██████▋   | 2/3 [20:54<11:28, 688.65s/it]\n",
            "1465it [07:53,  3.08it/s]\n",
            "1465it [07:53,  3.09it/s]\n",
            "\n",
            "1465it [07:53,  3.09it/s]\n",
            "\n",
            "1465it [07:58,  3.06it/s]\n",
            "1465it [07:58,  3.06it/s]\n",
            "\n",
            "\n",
            "\n",
            "1465it [07:58,  3.06it/s]\n",
            "100%|██████████| 3/3 [28:53<00:00, 625.75s/it]\n",
            "100%|██████████| 3/3 [28:57<00:00, 627.87s/it]\n",
            "100%|██████████| 3/3 [28:57<00:00, 579.22s/it]\n",
            "100%|██████████| 3/3 [28:57<00:00, 627.82s/it]\n",
            "100%|██████████| 3/3 [28:57<00:00, 579.30s/it]\n",
            "\n",
            "100%|██████████| 3/3 [28:53<00:00, 577.86s/it]\n",
            "100%|██████████| 3/3 [28:57<00:00, 579.26s/it]\n",
            "\n",
            "100%|██████████| 3/3 [28:59<00:00, 579.90s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training is Starting ...... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1465it [07:57,  3.07it/s]\n",
            "1465it [07:53,  3.09it/s]\n",
            "1465it [07:55,  3.08it/s]\n",
            "1465it [07:55,  3.08it/s]\n",
            "\n",
            "1465it [07:55,  3.08it/s]\n",
            "1465it [07:57,  3.07it/s]\n",
            "\n",
            "1465it [07:56,  3.08it/s]\n",
            "\n",
            "\n",
            "\n",
            "1465it [07:56,  3.07it/s]\n",
            "\n",
            "\n",
            "\n",
            "1465it [07:58,  3.06it/s]\n",
            "\n",
            "1465it [07:58,  3.06it/s]\n",
            "\n",
            "\n",
            "100%|██████████| 3/3 [23:52<00:00, 477.66s/it]\n",
            "\n",
            "100%|██████████| 3/3 [23:50<00:00, 476.87s/it]\n",
            "\n",
            "\n",
            "1465it [07:58,  3.06it/s]\n",
            "100%|██████████| 3/3 [23:53<00:00, 477.77s/it]\n",
            "1465it [07:58,  3.06it/s]\n",
            "100%|██████████| 3/3 [23:49<00:00, 476.43s/it]\n",
            "\n",
            "100%|██████████| 3/3 [23:51<00:00, 477.07s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training is Starting ...... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1465it [08:13,  2.97it/s]\n",
            "1465it [07:55,  3.08it/s]\n",
            "1465it [08:16,  2.95it/s]\n",
            "1465it [07:53,  3.09it/s]\n",
            "1465it [07:54,  3.09it/s]\n",
            "\n",
            "1465it [07:57,  3.07it/s]\n",
            "0it [00:00, ?it/s]\n",
            "1465it [07:58,  3.06it/s]\n",
            "1465it [07:58,  3.06it/s]\n",
            "\n",
            "1465it [07:58,  3.06it/s]\n",
            " 67%|██████▋   | 2/3 [16:15<08:11, 491.67s/it]\n",
            " 67%|██████▋   | 2/3 [15:55<07:57, 477.70s/it]\n",
            "1465it [07:58,  3.06it/s]\n",
            " 67%|██████▋   | 2/3 [16:01<08:01, 481.41s/it]\n",
            "1465it [07:59,  3.05it/s]\n",
            "1465it [07:59,  3.06it/s]\n",
            "1465it [07:59,  3.06it/s]\n",
            "1465it [07:59,  3.05it/s]\n",
            "\n",
            "\n",
            "100%|██████████| 3/3 [24:11<00:00, 486.27s/it]\n",
            "\n",
            "100%|██████████| 3/3 [24:14<00:00, 487.82s/it]\n",
            "100%|██████████| 3/3 [23:54<00:00, 477.57s/it]\n",
            "100%|██████████| 3/3 [24:00<00:00, 480.29s/it]\n",
            "100%|██████████| 3/3 [23:54<00:00, 478.02s/it]\n",
            "\n",
            "\n",
            "100%|██████████| 3/3 [23:55<00:00, 478.52s/it]\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training is Starting ...... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1465it [07:54,  3.09it/s]\n",
            "1465it [07:55,  3.08it/s]\n",
            "1465it [07:54,  3.09it/s]\n",
            " 33%|███▎      | 1/3 [07:54<15:48, 474.34s/it]\n",
            "0it [00:00, ?it/s]\n",
            "1465it [08:02,  3.04it/s]\n",
            "\n",
            "\n",
            "1465it [07:57,  3.07it/s]\n",
            "\n",
            "1465it [07:57,  3.07it/s]\n",
            "\n",
            "1465it [07:56,  3.01it/s]\n",
            "\n",
            "1465it [07:56,  3.07it/s]\n",
            "\n",
            "1465it [08:01,  3.05it/s]\n",
            "\n",
            "1465it [08:00,  3.05it/s]\n",
            "1465it [08:01,  3.05it/s]\n",
            "\n",
            "1465it [08:01,  2.92it/s]\n",
            "100%|██████████| 3/3 [24:18<00:00, 489.59s/it]\n",
            "\n",
            "\n",
            "100%|██████████| 3/3 [24:18<00:00, 486.13s/it]\n",
            "\n",
            "100%|██████████| 3/3 [23:52<00:00, 477.59s/it]\n",
            "1465it [08:00,  3.05it/s]\n",
            "\n",
            "100%|██████████| 3/3 [24:00<00:00, 480.33s/it]\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training is Starting ...... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1465it [07:55,  3.08it/s]\n",
            "1465it [07:52,  3.10it/s]\n",
            "1465it [07:55,  3.08it/s]\n",
            "\n",
            "\n",
            "\n",
            "1465it [08:14,  2.96it/s]\n",
            "\n",
            "1465it [07:55,  3.08it/s]\n",
            "\n",
            "\n",
            "1465it [07:55,  3.08it/s]\n",
            "1465it [07:55,  3.08it/s]\n",
            "1465it [07:55,  3.08it/s]\n",
            "1465it [07:55,  3.08it/s]\n",
            "\n",
            "1465it [08:00,  3.05it/s]\n",
            "1465it [08:00,  3.05it/s]\n",
            "1465it [08:00,  3.05it/s]\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 3/3 [23:50<00:00, 476.22s/it]\n",
            "\n",
            "100%|██████████| 3/3 [23:51<00:00, 477.01s/it]\n",
            "\n",
            "100%|██████████| 3/3 [23:51<00:00, 477.26s/it]\n",
            "\n",
            "100%|██████████| 3/3 [23:51<00:00, 477.04s/it]\n",
            "100%|██████████| 3/3 [24:11<00:00, 486.51s/it]\n",
            "100%|██████████| 3/3 [23:51<00:00, 477.28s/it]\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZN73Nb9siLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596c691f-0b21-401a-a2d3-9bf4ff6e3fa3"
      },
      "source": [
        "def train_all(train_loader, model, device, optimizer):\r\n",
        "    model.train()\r\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\r\n",
        "    model.train()\r\n",
        "    lss=bce()\r\n",
        "    loss1=[]\r\n",
        "    for step, (x, y_batch) in enumerate(train_loader): \r\n",
        "            \r\n",
        "            # x = x.to(device)\r\n",
        "            y_batch = y_batch.to(device)\r\n",
        "            y_pred = model(x)\r\n",
        "            \r\n",
        "            loss = lss(y_pred.view(-1).float(), y_batch.float())\r\n",
        "            loss.backward()\r\n",
        "            loss1.append(loss.item())\r\n",
        "            optimizer.step()\r\n",
        "            \r\n",
        "            model.zero_grad()\r\n",
        "    return mean(loss1)\r\n",
        "\r\n",
        "def valid_all(train_loader, model, device):\r\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\r\n",
        "    lss=bce()\r\n",
        "    loss1=[]\r\n",
        "    for step, (x, y_batch) in enumerate(train_loader): \r\n",
        "            \r\n",
        "            # x = x.to(device)\r\n",
        "            y_batch = y_batch.to(device)\r\n",
        "            y_pred = model(x)\r\n",
        "            \r\n",
        "            loss = lss(y_pred.view(-1).float(), y_batch.float())\r\n",
        "            loss1.append(loss.item())\r\n",
        "            \r\n",
        "    return mean(loss1)\r\n",
        "\r\n",
        "def predict_all(train_loader, model,device,df,batch_size):\r\n",
        "    # t = tqdm(train_loader, disable=not xm.is_master_ordinal())\r\n",
        "    predict=[]\r\n",
        "    for step, (x) in tqdm(enumerate(train_loader),total=df.shape[0]/(batch_size),position=0,leave=True): \r\n",
        "            \r\n",
        "            y_pred = model(x.to(device))\r\n",
        "            predict.append(y_pred.cpu().detach().numpy())\r\n",
        "            \r\n",
        "    return predict\r\n",
        "\r\n",
        "def load_data(lang):\r\n",
        "    tst=pd.read_csv('/content/gdrive/My Drive/multilingual/test.csv.zip',usecols=['lang','content']) \r\n",
        "    tst=tst.loc[tst['lang']==lang].reset_index(drop=True).drop(['lang'],1)\r\n",
        "    print(tst.shape)\r\n",
        "    # tst = tst.iloc[:1000]\r\n",
        "    return tst\r\n",
        "\r\n",
        "\r\n",
        "def get_lang(val,tst,lang):\r\n",
        "  df=pd.concat([val,tst],0)\r\n",
        "  return df.loc[df['lang']==lang].reset_index(drop=True).drop(['id','lang'],1)\r\n",
        "\r\n",
        "def main():\r\n",
        "    epochs=1\r\n",
        "    seed=42\r\n",
        "    batch_size=16\r\n",
        "    # Setting seed\r\n",
        "    random.seed(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "    l1='fr'\r\n",
        "    lang='french'\r\n",
        "    link_dk={'fr':'camembert-base',\r\n",
        "             'pt':'neuralmind/bert-base-portuguese-cased',\r\n",
        "             'ru':'DeepPavlov/rubert-base-cased',\r\n",
        "             'tr':'dbmdz/bert-base-turkish-cased',\r\n",
        "             'es':'dccuchile/bert-base-spanish-wwm-cased',\r\n",
        "             'it':'dbmdz/bert-base-italian-xxl-cased'\r\n",
        "             }\r\n",
        "    df=load_data(l1)\r\n",
        "    tokenizer = AutoTokenizer.from_pretrained(link_dk[l1])\r\n",
        "    x_train = regular_encode(list(df.content.values), tokenizer, maxlen=192)\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def run():\r\n",
        "\r\n",
        "        torch.manual_seed(seed)\r\n",
        "\r\n",
        "        device = xm.xla_device()\r\n",
        "        model = AutoModel.from_pretrained(link_dk[l1])\r\n",
        "        model=Transformer(model).to(device)\r\n",
        "        model.load_state_dict(torch.load( lang+str(number))['state_dict'])\r\n",
        "\r\n",
        "        #Training\r\n",
        "        train_dataset = JigsawDataset(x_train,None,1)\r\n",
        "\r\n",
        "\r\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(\r\n",
        "            train_dataset,\r\n",
        "            num_replicas=xm.xrt_world_size(),\r\n",
        "            rank=xm.get_ordinal(),\r\n",
        "            shuffle=False\r\n",
        "        )\r\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,\r\n",
        "            sampler=train_sampler,\r\n",
        "            drop_last=False,\r\n",
        "            num_workers=2\r\n",
        "        )\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "        predictions=[]\r\n",
        "        para_loader = pl.ParallelLoader(train_loader, [device])\r\n",
        "        predictions.append(predict_all(para_loader.per_device_loader(device),model,device,df,batch_size))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        np.save('/content/gdrive/My Drive/multilingual/'+lang+'_predictions_'+str(number)+'.npy',predictions)\r\n",
        "            \r\n",
        "    for number in range(1,6):\r\n",
        "      run()\r\n",
        "main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10920, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "683it [01:47,  6.38it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n",
            "683it [01:31,  7.47it/s]\n",
            "683it [01:31,  7.44it/s]\n",
            "683it [01:31,  7.44it/s]\n",
            "683it [01:31,  7.44it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uywbMB3w2MZl"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}