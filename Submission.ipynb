{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import warnings\n",
        "import glob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
        "\n",
        "import gc\n",
        "import shutil\n",
        "import zipfile\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda import amp\n",
        "from torchinfo import summary\n",
        "from torchmetrics import MeanMetric, MulticlassAccuracy, AveragePrecision\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import base64\n",
        "import typing as t\n",
        "import zlib"
      ],
      "metadata": {
        "id": "Q0oylnDdp3aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TLpMOKjho6i"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a directory to store the large image tiles.\n",
        "os.mkdir(\"big_ones\")\n",
        "\n",
        "# Load the tile metadata from the CSV file.\n",
        "tile_meta_path = \"/kaggle/input/hubmap-hacking-the-human-vasculature/tile_meta.csv\"\n",
        "tile_metadata = pd.read_csv(tile_meta_path)\n",
        "\n",
        "# Group the metadata by the source whole slide image (wsi).\n",
        "for wsi, group in tile_metadata.groupby(['source_wsi']):\n",
        "    # Sort the tiles based on their coordinates.\n",
        "    group = group.sort_values(by=['i', 'j'])\n",
        "    group['i'] = group['i'] - min(group['i'])\n",
        "    group['j'] = group['j'] - min(group['j'])\n",
        "    max_x = group['i'].max()\n",
        "    max_y = group['j'].max()\n",
        "\n",
        "    # Create a large canvas to combine the tiles.\n",
        "    big_one = np.zeros((max_y + 512, max_x + 512, 3))\n",
        "\n",
        "    # Iterate through each tile and paste it onto the large canvas.\n",
        "    for _, tile_row in enumerate(tqdm(group.iterrows())):\n",
        "        tile = tile_row[1]\n",
        "        path = '/kaggle/input/hubmap-hacking-the-human-vasculature/test/' + tile['id'] + '.tif'\n",
        "\n",
        "        # Check if the tile image file exists.\n",
        "        if os.path.isfile(path):\n",
        "            img = cv2.imread(path)\n",
        "            x, y = tile['i'], tile['j']\n",
        "            big_one[y:y + 512, x:x + 512, :] = img\n",
        "\n",
        "    # Save the combined large image if it contains non-zero data.\n",
        "    if np.sum(big_one) > 0:\n",
        "        large_image_path = os.path.join(\"big_ones\", str(wsi) + '.jpg')\n",
        "        cv2.imwrite(large_image_path, big_one)\n",
        "\n",
        "# Load the tile metadata again for further processing.\n",
        "meta = pd.read_csv(tile_meta_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tile_data(data):\n",
        "    \"\"\"\n",
        "    Preprocesses the tile data to adjust coordinates and create new features.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): DataFrame containing tile metadata.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Preprocessed tile data.\n",
        "    \"\"\"\n",
        "    # Adjusting coordinates to be relative to minimum values.\n",
        "    data['i'] = data['i'] - data['i'].min()\n",
        "    data['j'] = data['j'] - data['j'].min()\n",
        "\n",
        "    # Calculate end coordinates and other features.\n",
        "    data['i_end'] = data['i'].max()\n",
        "    data['j_end'] = data['j'].max()\n",
        "    data['big_i_start'] = data['i'] - 512\n",
        "    data['big_j_start'] = data['j'] - 512\n",
        "    data['i_start'] = 512\n",
        "    data['j_start'] = 512\n",
        "    data['big_i_end'] = data['i'] + 1024\n",
        "    data['big_j_end'] = data['j'] + 1024\n",
        "\n",
        "    # Adjust coordinates for tiles touching image borders.\n",
        "    data.loc[data['big_i_start'] < 0, 'big_i_end'] += 512\n",
        "    data.loc[data['big_i_start'] < 0, 'i_start'] -= 512\n",
        "    data.loc[data['big_i_start'] < 0, 'big_i_start'] += 512\n",
        "\n",
        "    data.loc[data['big_j_start'] < 0, 'big_j_end'] += 512\n",
        "    data.loc[data['big_j_start'] < 0, 'j_start'] -= 512\n",
        "    data.loc[data['big_j_start'] < 0, 'big_j_start'] += 512\n",
        "\n",
        "    data.loc[(data['big_i_end'] - data['i_end']) > 512, 'big_i_start'] -= 512\n",
        "    data.loc[(data['big_i_end'] - data['i_end']) > 512, 'i_start'] += 512\n",
        "    data.loc[(data['big_i_end'] - data['i_end']) > 512, 'big_i_end'] -= 512\n",
        "\n",
        "    data.loc[(data['big_j_end'] - data['j_end']) > 512, 'big_j_start'] -= 512\n",
        "    data.loc[(data['big_j_end'] - data['j_end']) > 512, 'j_start'] += 512\n",
        "    data.loc[(data['big_j_end'] - data['j_end']) > 512, 'big_j_end'] -= 512\n",
        "\n",
        "    return data\n",
        "\n",
        "# Load the tile metadata from the CSV file.\n",
        "tile_meta_path = \"/kaggle/input/hubmap-hacking-the-human-vasculature/tile_meta.csv\"\n",
        "tile_metadata = pd.read_csv(tile_meta_path)\n",
        "\n",
        "# Apply preprocessing function to each group of tile data.\n",
        "df = tile_metadata.groupby('source_wsi').apply(preprocess_tile_data)\n",
        "\n",
        "selected_columns = [\n",
        "    'source_wsi', 'id', 'big_i_start', 'big_j_start', 'i_start', 'j_start',\n",
        "    'big_i_end', 'big_j_end', 'i', 'j', 'i_end', 'j_end'\n",
        "]\n",
        "\n",
        "renamed_columns = [\n",
        "    'source_wsi', 'id', 'big_image_i_start', 'big_image_j_start', 'image_i_start', 'image_j_start',\n",
        "    'big_image_i_end', 'big_image_j_end', 'i', 'j', 'i_end', 'j_end'\n",
        "]\n",
        "\n",
        "# Reorder the DataFrame columns based on the selected columns and renamed columns.\n",
        "df = df[selected_columns]\n",
        "df.columns = renamed_columns\n"
      ],
      "metadata": {
        "id": "vSvLW0kppqsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DroneDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for processing drone images.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, processor, preds):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Dataframe containing information about images.\n",
        "            processor (SegformerImageProcessor): Processor for image preprocessing.\n",
        "            preds (list): List of file paths to predictions.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.path = preds\n",
        "        self.processor = processor\n",
        "        self.big_one = {}\n",
        "        for path in glob.glob(\"/kaggle/working/big_ones/*\"):\n",
        "            ids = path.split(\"/\")[-1].split(\".\")[0]\n",
        "            self.big_one[ids] = cv2.imread(\"big_ones/\" + str(ids) + \".jpg\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the length of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Length of the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Get an item from the dataset.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index of the item.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple containing image ID, preprocessed image tensor, and coordinates.\n",
        "        \"\"\"\n",
        "        path = self.path[index]\n",
        "        ids = path.split(\"/\")[-1].split(\".\")[0]\n",
        "        info = self.df.loc[self.df['id'] == ids]\n",
        "\n",
        "        x1, y1 = 0, 0\n",
        "        if info.shape[0] > 0:\n",
        "            val = info[[\"source_wsi\", \"big_image_i_start\", \"big_image_j_start\", \"big_image_i_end\", \"big_image_j_end\", \"image_i_start\",\"image_j_start\", 'i','j','i_end','j_end']].values\n",
        "            wsi, big_image_y_start, big_image_x_start, big_image_y_end, big_image_x_end, start_y, start_x,_,_,_,_ = val[0]\n",
        "            big_one = self.big_one[str(wsi)]\n",
        "            big_one = big_one[big_image_x_start : big_image_x_end, big_image_y_start : big_image_y_end, : ]\n",
        "            image = big_one\n",
        "            x1, y1 = start_x, start_y\n",
        "        else:\n",
        "            image = cv2.imread(path)\n",
        "\n",
        "        encoded_inputs = self.processor.preprocess(\n",
        "                images=image,\n",
        "                return_tensors=\"pt\", # Return pytorch tensor.\n",
        "                do_rescale=True,\n",
        "                rescale_factor=1.0 / 255,\n",
        "                do_normalize=True,\n",
        "                image_mean=(0.485, 0.456, 0.406),\n",
        "                image_std=(0.229, 0.224, 0.225),\n",
        "                resample=2,\n",
        "            )\n",
        "        image = encoded_inputs[\"pixel_values\"].squeeze_()\n",
        "        return ids, image, x1, y1\n",
        "\n",
        "# Instantiate the SegformerImageProcessor.\n",
        "processor = SegformerImageProcessor(\n",
        "    do_resize=False,\n",
        "    do_rescale=False,\n",
        "    do_normalize=False,\n",
        ")\n",
        "\n",
        "# Create the DroneDataset using the provided dataframe and file paths.\n",
        "dataset = DroneDataset(df, processor, glob.glob(\"/kaggle/input/hubmap-hacking-the-human-vasculature/test/*\"))\n",
        "\n",
        "# Get the keys of the 'big_one' dictionary in the dataset.\n",
        "keys = dataset.big_one.keys()\n",
        "print(keys)"
      ],
      "metadata": {
        "id": "VXxiqRbeppBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_default_device():\n",
        "    \"\"\"\n",
        "    Get the default device for PyTorch.\n",
        "\n",
        "    Returns:\n",
        "        torch.device: Default device (cuda if available, else cpu).\n",
        "        bool: Flag indicating if GPU is available.\n",
        "    \"\"\"\n",
        "    gpu_available = torch.cuda.is_available()\n",
        "    return torch.device('cuda' if gpu_available else 'cpu'), gpu_available\n",
        "\n",
        "def seed_everything(seed_value):\n",
        "    \"\"\"\n",
        "    Seed various random number generators for reproducibility.\n",
        "\n",
        "    Args:\n",
        "        seed_value (int): Seed value.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Define data configurations\n",
        "\n",
        "@dataclass\n",
        "class DatasetConfig:\n",
        "    \"\"\"\n",
        "    Data configuration class.\n",
        "    \"\"\"\n",
        "    NUM_CLASSES: int = 4\n",
        "    IMG_WIDTH: int = 512\n",
        "    IMG_HEIGHT: int = 512\n",
        "    DATA_TRAIN_IMAGES: list = [\n",
        "        \"/kaggle/input/final-dataset/big_ones/1.jpg\",\n",
        "        \"/kaggle/input/final-dataset/big_ones/2.jpg\",\n",
        "        \"/kaggle/input/final-dataset/big_ones/3.jpg\"\n",
        "    ]\n",
        "    DATA_VALID_IMAGES: list = [\n",
        "        '/kaggle/input/final-dataset/big_ones/4.jpg'\n",
        "    ]\n",
        "    MEAN: tuple = (0.485, 0.456, 0.406)\n",
        "    STD: tuple = (0.229, 0.224, 0.225)\n",
        "    BACKGROUND_CLS_ID: int = 0\n",
        "\n",
        "# Define training configuration\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"\n",
        "    Training configuration class.\n",
        "    \"\"\"\n",
        "    BATCH_SIZE: int = 1\n",
        "    NUM_EPOCHS: int = 1\n",
        "    LEARNING_RATE: float = 1e-4\n",
        "    NUM_WORKERS: int = 1\n",
        "    WEIGHT_DECAY: float = 1e-4\n",
        "\n",
        "# Define inference configuration\n",
        "\n",
        "@dataclass\n",
        "class InferenceConfig:\n",
        "    \"\"\"\n",
        "    Inference configuration class.\n",
        "    \"\"\"\n",
        "    BATCH_SIZE: int = 1\n",
        "    NUM_BATCHES: int = 1\n",
        "\n",
        "# Define model configuration\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"\n",
        "    Model configuration class.\n",
        "    \"\"\"\n",
        "    MODEL_NAME: str = \"nvidia/segformer-b3-finetuned-ade-512-512\"\n",
        "\n",
        "# Define custom functions for image and mask processing\n",
        "\n",
        "def get_bounding_box(ground_truth_map):\n",
        "    \"\"\"\n",
        "    Get the bounding box coordinates from a mask.\n",
        "\n",
        "    Args:\n",
        "        ground_truth_map (np.ndarray): Ground truth mask.\n",
        "\n",
        "    Returns:\n",
        "        list: Bounding box coordinates [x_min, y_min, x_max, y_max].\n",
        "    \"\"\"\n",
        "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
        "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
        "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
        "    return [x_min, y_min, x_max, y_max]\n",
        "\n",
        "def rle_decode(mask_rle, shape, color=1):\n",
        "    \"\"\"\n",
        "    Decode a run-length encoded mask.\n",
        "\n",
        "    Args:\n",
        "        mask_rle (str): Run-length encoded mask.\n",
        "        shape (tuple): Shape of the output mask.\n",
        "        color (int, optional): Color for the mask. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Decoded mask.\n",
        "    \"\"\"\n",
        "    s = mask_rle.split()\n",
        "    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n",
        "    lengths = list(map(int, s[1::2]))\n",
        "    ends = [x + y for x, y in zip(starts, lengths)]\n",
        "    if len(shape) == 3:\n",
        "        img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n",
        "    else:\n",
        "        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n",
        "    for start, end in zip(starts, ends):\n",
        "        img[start : end] = color\n",
        "    return img.reshape(shape)\n",
        "\n",
        "def get_box(a_mask):\n",
        "    \"\"\"\n",
        "    Get the bounding box of a given mask.\n",
        "\n",
        "    Args:\n",
        "        a_mask (np.ndarray): Mask.\n",
        "\n",
        "    Returns:\n",
        "        list: Bounding box coordinates [x_min, y_min, x_max, y_max].\n",
        "    \"\"\"\n",
        "    pos = np.where(a_mask)\n",
        "    xmin = np.min(pos[1])\n",
        "    xmax = np.max(pos[1])\n",
        "    ymin = np.min(pos[0])\n",
        "    ymax = np.max(pos[0])\n",
        "    return [xmin, ymin, xmax, ymax]\n",
        "\n",
        "\n",
        "# Create a mapping of class ID to RGB value.\n",
        "id2color = {0:(0,0,0),1:(255,0,0),2:(0,255,0),3:(0,0,255)}\n",
        "\n",
        "# del id2color[23] # To remove the 'conflicting' class.\n",
        "\n",
        "DatasetConfig.NUM_CLASSES = len(id2color)\n",
        "# Reverse id2color mapping.\n",
        "# Used for converting RGB mask to a single channel (grayscale) representation.\n",
        "rev_id2color = {value: key for key, value in id2color.items()}\n",
        "def rgb_to_grayscale(rgb_arr, color_map=rev_id2color, background_cls_id=0):\n",
        "\n",
        "    # Collapse H, W dimensions.\n",
        "    reshaped_rgb_arr = rgb_arr.reshape((-1, 3))\n",
        "\n",
        "    # Get an array of all unique pixels along with the \"inverse\" array\n",
        "    # (of the same shape as the original array) filled with indices to the unique array.\n",
        "    # Each value in the \"inverse\" array points to the unique pixel at that\n",
        "    # location in the input array.\n",
        "    unique_pixels, inverse = np.unique(reshaped_rgb_arr, axis=0, return_inverse=True)\n",
        "\n",
        "    # If a unique pixel is not found in the color_map, class ID of background pixel is used.\n",
        "    grayscale_map = np.array([color_map.get(tuple(pixel), background_cls_id) for pixel in unique_pixels])[inverse]\n",
        "\n",
        "    return grayscale_map.reshape(rgb_arr.shape[:2])\n",
        "def num_to_rgb(num_arr, color_map=id2color):\n",
        "    single_layer = np.squeeze(num_arr)\n",
        "    output = np.zeros(num_arr.shape[:2] + (3,))\n",
        "\n",
        "    for k in color_map.keys():\n",
        "        output[single_layer == k] = color_map[k]\n",
        "\n",
        "    return np.float32(output) / 255.0 # return a floating point array in range [0.0, 1.0]\n",
        "def image_overlay(image, segmented_image):\n",
        "\n",
        "    alpha = 1.0 # Transparency for the original image.\n",
        "    beta  = 0.7 # Transparency for the segmentation map.\n",
        "    gamma = 0.0 # Scalar added to each sum.\n",
        "\n",
        "    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    image = cv2.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return np.clip(image, 0.0, 1.0)\n",
        "def display_image_and_mask(*, images, masks, color_mask=False, color_map=id2color):\n",
        "    title = [\"GT Image\", \"GT Mask\", \"Color Mask\", \"Overlayed Mask\"]\n",
        "\n",
        "    for idx in range(images.shape[0]):\n",
        "        image = images[idx]\n",
        "        grayscale_gt_mask = masks[idx]\n",
        "\n",
        "        fig = plt.figure(figsize=(15, 4))\n",
        "\n",
        "        # Create RGB segmentation map from grayscale segmentation map.\n",
        "        rgb_gt_mask = num_to_rgb(grayscale_gt_mask, color_map=color_map)\n",
        "\n",
        "        # Create the overlayed image.\n",
        "        overlayed_image = image_overlay(image, rgb_gt_mask)\n",
        "\n",
        "        plt.subplot(1, 4, 1)\n",
        "        plt.title(title[0])\n",
        "        plt.imshow(image)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 4, 2)\n",
        "        plt.title(title[1])\n",
        "        plt.imshow(grayscale_gt_mask, cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 4, 3)\n",
        "        plt.title(title[2])\n",
        "        plt.imshow(rgb_gt_mask)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.imshow(rgb_gt_mask)\n",
        "        plt.subplot(1, 4, 4)\n",
        "        plt.title(title[3])\n",
        "        plt.imshow(overlayed_image)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    return\n",
        "def get_model():\n",
        "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "        \"/kaggle/input/refbsrb/abc\",\n",
        "        num_labels=4,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    )\n",
        "    return model\n",
        "model = get_model()\n",
        "model.load_state_dict(torch.load(\"/kaggle/input/weights-1560/1/best_cross_entropy.pth\",map_location=torch.device('cpu')))\n"
      ],
      "metadata": {
        "id": "sHZ1f33jpnMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9a0B5oNho6j"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define utility functions\n",
        "\n",
        "def save_model(name, mod):\n",
        "    \"\"\"\n",
        "    Save the model's state dictionary to a file.\n",
        "\n",
        "    Args:\n",
        "        name (str): File name.\n",
        "        mod (nn.Module): PyTorch model.\n",
        "    \"\"\"\n",
        "    torch.save(mod.state_dict(), name)\n",
        "\n",
        "def mapping(pred):\n",
        "    \"\"\"\n",
        "    Map the predicted labels to RGB colors.\n",
        "\n",
        "    Args:\n",
        "        pred (np.ndarray): Predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: RGB image.\n",
        "    \"\"\"\n",
        "    dk = {0: [0, 0, 0], 1: [255, 0, 0], 2: [0, 255, 0], 3: [0, 0, 255]}\n",
        "    img = np.zeros((512, 512, 3))\n",
        "    if len(pred.shape) == 3:\n",
        "        pred = np.argmax(pred, 0)\n",
        "    for i in range(512):\n",
        "        for j in range(512):\n",
        "            img[i, j, :] = dk[pred[i, j]]\n",
        "    return img\n",
        "\n",
        "def evaluate(model, loader, device, num_classes, epoch_idx, criteria, total_epochs, validation=True):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the validation/test dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): PyTorch model.\n",
        "        loader (DataLoader): Data loader.\n",
        "        device (torch.device): Device (cuda or cpu).\n",
        "        num_classes (int): Number of classes.\n",
        "        epoch_idx (int): Current epoch index.\n",
        "        criteria: Loss function.\n",
        "        total_epochs (int): Total number of epochs.\n",
        "        validation (bool, optional): Flag for validation set. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        dict: Model outputs.\n",
        "    \"\"\"\n",
        "    # Change model mode.\n",
        "    model.eval()\n",
        "\n",
        "    loss_record = MeanMetric()\n",
        "    metric_record = MeanMetric()\n",
        "    acc_record = MulticlassAccuracy(num_classes=num_classes, average=\"micro\")\n",
        "\n",
        "    loader_len = len(loader)\n",
        "\n",
        "    with tqdm(total=loader_len, ncols=122, ascii=True) as tq:\n",
        "        tq.set_description(f\"{'Valid' if validation else 'Test'} :: Epoch: {epoch_idx}/{total_epochs}\")\n",
        "\n",
        "        for en, (data, target) in enumerate(loader):\n",
        "            tq.update(1)\n",
        "\n",
        "            # Send data and target to GPU device if available.\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            with torch.no_grad():\n",
        "                # Perform Forward pass through the model. Output is a dictionary.\n",
        "                outputs = model(pixel_values=data, return_dict=True)\n",
        "\n",
        "            logits = outputs['logits']\n",
        "            upsampled_logits = nn.functional.interpolate(logits, size=target.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "            rgb_pred_mask = mapping(upsampled_logits.squeeze(0).detach().cpu().numpy())\n",
        "            image = data.cpu().numpy()\n",
        "\n",
        "            image = image[0].transpose(1, 2, 0)\n",
        "\n",
        "            fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n",
        "            axes[0, 0].imshow(image)\n",
        "            axes[0, 1].imshow(rgb_pred_mask)\n",
        "            axes[1, 0].imshow(0.8 * image + 0.2 * rgb_pred_mask)\n",
        "            axes[1, 1].imshow(mapping(target.squeeze(0).cpu().numpy()))\n",
        "            fig.tight_layout()\n",
        "            plt.savefig(\"/kaggle/working/\" + str(en) + \".jpg\")\n",
        "            plt.show()\n",
        "\n",
        "    return outputs\n",
        "\n",
        "def main(*, model, optimizer, ckpt_path, configs=None, pin_memory=False, device=\"cpu\"):\n",
        "\n",
        "    # Create Dataloader.\n",
        "    train_loader, _, valid_loader, _ = get_dataloader(configs=configs, pin_memory=pin_memory, num_workers=TrainingConfig.NUM_WORKERS)\n",
        "    plotting={\n",
        "                \"loss\": [],\n",
        "                \"val_loss\": [],\n",
        "                \"accuracy\": [],\n",
        "                \"val_accuracy\": [],\n",
        "                \"IoU\": [],\n",
        "                \"val_IoU\": []\n",
        "    }\n",
        "    # Intialize learning rate scheduler.\n",
        "    if configs[\"LR_SCHEDULER\"]:\n",
        "        milestones = [configs[\"NUM_EPOCHS\"] // 2,]  # Decrease LR by 0.1 after 50% of traiining.\n",
        "        configs[\"SCHLR_MILESTONES\"] = milestones\n",
        "\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=configs[\"SCHLR_MILESTONES\"], gamma=0.1)\n",
        "\n",
        "\n",
        "    # Creates a GradScaler once at the beginning of training\n",
        "    # for Automatic Mixed-Precision training.\n",
        "    scaler = amp.GradScaler()\n",
        "    FL = FocalLoss()\n",
        "\n",
        "    # Save the model if validation loss improves.\n",
        "    best_valid_loss = float(\"inf\")\n",
        "\n",
        "    # Plot training and validation epoch logs.\n",
        "    v_l = 100\n",
        "    v_iou = 0\n",
        "    v_a = 0\n",
        "    # Training Loop.\n",
        "    for epoch in range(configs[\"NUM_EPOCHS\"]):\n",
        "        # Memory Cleanup.\n",
        "        gc.collect()\n",
        "\n",
        "        output = evaluate(\n",
        "            model=model,\n",
        "            loader=valid_loader,\n",
        "            device=device,\n",
        "            criteria = FL,\n",
        "            num_classes=configs[\"NUM_CLASSES\"],\n",
        "            epoch_idx=epoch + 1,\n",
        "            total_epochs=configs[\"NUM_EPOCHS\"],\n",
        "        )\n",
        "\n",
        "    return output\n",
        "# For deterministic training\n",
        "seed_everything(seed_value=41)\n",
        "\n",
        "# Set default device to GPU if available.\n",
        "DEVICE, GPU_AVAILABLE = get_default_device()\n",
        "# Create a model.\n",
        "model = get_model()\n",
        "\n",
        "# Send model to the device (GPU/CPU)\n",
        "model.to(DEVICE);\n",
        "LR = TrainingConfig.LEARNING_RATE\n",
        "WD = TrainingConfig.WEIGHT_DECAY\n",
        "optimizer = getattr(torch.optim, \"AdamW\")(model.parameters(), lr=LR, weight_decay=WD, amsgrad=True)\n",
        "\n",
        "HPARAMS={}\n",
        "HPARAMS['IMG_SIZE']      = (DatasetConfig.IMG_HEIGHT, DatasetConfig.IMG_WIDTH)\n",
        "HPARAMS['MODEL_NAME']    = ModelConfig.MODEL_NAME\n",
        "HPARAMS['BATCH_SIZE']    = TrainingConfig.BATCH_SIZE\n",
        "HPARAMS['NUM_EPOCHS']    = TrainingConfig.NUM_EPOCHS\n",
        "\n",
        "HPARAMS['OPTIMIZER']     = \"AdamW\"\n",
        "HPARAMS['LEARNING_RATE'] = TrainingConfig.LEARNING_RATE\n",
        "HPARAMS['WEIGHT_DECAY']  = TrainingConfig.WEIGHT_DECAY\n",
        "HPARAMS['LR_SCHEDULER']  = \"MultiStepLR\"\n",
        "HPARAMS['NUM_CLASSES'] = 4\n",
        "train_loader=dataset\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, ignore_index=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Compute the softmax over the inputs\n",
        "        inputs_soft = F.softmax(inputs, dim=1)\n",
        "\n",
        "        # Create the labels for the softmax using one-hot encoding\n",
        "        target_one_hot = F.one_hot(targets, num_classes=inputs.shape[1]).permute(0, 3, 1, 2).float()\n",
        "\n",
        "        # Compute the focal loss\n",
        "        focal = -self.alpha * ((1 - inputs_soft)**self.gamma) * target_one_hot * torch.log(inputs_soft.clamp(min=1e-8))\n",
        "\n",
        "        # Mask the pixels to ignore\n",
        "        if self.ignore_index is not None:\n",
        "            mask = targets != self.ignore_index\n",
        "            focal = focal * mask.unsqueeze(1).float()\n",
        "\n",
        "        # Reduce the loss\n",
        "        if self.reduction == 'mean':\n",
        "            focal = focal.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            focal = focal.sum()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid reduction mode: {self.reduction}\")\n",
        "\n",
        "        return focal\n",
        "model.load_state_dict(torch.load(\"/kaggle/input/weights-1560/3/best_miou.pth\",map_location=torch.device('cpu')))\n",
        "\n",
        "class CFG:\n",
        "    data_path = '/kaggle/input/hubmap-hacking-the-human-vasculature/'\n",
        "    batch_size = 1\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    th = 0.15\n",
        "    chepoint_dir = '/kaggle/input/hubmap-checpoint/'\n",
        "    model_types = ['UnetPlusPlus', 'UnetPlusPlus', 'UnetPlusPlus']\n",
        "    encoder_name_list = ['se_resnext50_32x4d', 'se_resnext101_32x4d', 'vgg19_bn']\n",
        "    is_tta = False\n",
        "    size = 512\n",
        "    org_size = 512\n",
        "    encoder_depth = 4\n",
        "    decoder_channels = [512, 256, 128, 64]\n",
        "\n",
        "    test_aug = [\n",
        "        A.Resize(size, size),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        "model.to(CFG.device)\n",
        "model=model.float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFNztOqQho6j"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!mkdir /kaggle/working/packages\n",
        "!cp -r /kaggle/input/pycocotools/* /kaggle/working/packages\n",
        "os.chdir(\"/kaggle/working/packages/pycocotools-2.0.6/\")\n",
        "!python setup.py install\n",
        "!pip install . --no-index --find-links /kaggle/working/packages/\n",
        "os.chdir(\"/kaggle/working\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu_6iNpCho6j"
      },
      "outputs": [],
      "source": [
        "from pycocotools import _mask as coco_mask\n",
        "\n",
        "def TTA(x: torch.Tensor, model: nn.Module):\n",
        "    \"\"\"\n",
        "    Apply Test Time Augmentation (TTA) to input tensor x using the given model.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Input tensor.\n",
        "        model (nn.Module): Model used for predictions.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: TTA-enhanced predictions.\n",
        "    \"\"\"\n",
        "    shape = x.shape\n",
        "    x = [model(torch.rot90(x, k=i, dims=(-2, -1)))['logits'] for i in range(4)]\n",
        "    x = [torch.rot90(x[i], k=-i, dims=(-2, -1)) for i in range(4)]\n",
        "    x = torch.stack(x, dim=0)\n",
        "    return torch.max(x, 0).values\n",
        "\n",
        "class Test:\n",
        "    \"\"\"\n",
        "    Class for testing and evaluating a model on a dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def encode_binary_mask(self, mask: np.ndarray) -> t.Text:\n",
        "        \"\"\"\n",
        "        Convert a binary mask into OID challenge encoding ascii text.\n",
        "\n",
        "        Args:\n",
        "            mask (np.ndarray): Binary mask to encode.\n",
        "\n",
        "        Returns:\n",
        "            t.Text: Encoded binary mask as ascii text.\n",
        "        \"\"\"\n",
        "        # check input mask --\n",
        "        if mask.dtype != np.bool:\n",
        "            raise ValueError(\n",
        "                \"encode_binary_mask expects a binary mask, received dtype == %s\" %\n",
        "                mask.dtype)\n",
        "\n",
        "        mask = np.squeeze(mask)\n",
        "        if len(mask.shape) != 2:\n",
        "            raise ValueError(\n",
        "                \"encode_binary_mask expects a 2d mask, received shape == %s\" %\n",
        "                mask.shape)\n",
        "\n",
        "        # convert input mask to expected COCO API input --\n",
        "        mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
        "        mask_to_encode = mask_to_encode.astype(np.uint8)\n",
        "        mask_to_encode = np.asfortranarray(mask_to_encode)\n",
        "\n",
        "        # RLE encode mask --\n",
        "        encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n",
        "\n",
        "        # compress and base64 encoding --\n",
        "        binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n",
        "        base64_str = base64.b64encode(binary_str)\n",
        "        return base64_str\n",
        "\n",
        "    def encode_output(self, outputs, idx):\n",
        "        \"\"\"\n",
        "        Encode the outputs into challenge-specific format.\n",
        "\n",
        "        Args:\n",
        "            outputs: Model predictions.\n",
        "            idx: Index of the prediction.\n",
        "\n",
        "        Returns:\n",
        "            dict: Encoded outputs in challenge format.\n",
        "        \"\"\"\n",
        "        blood_vessel = torch.argmax(outputs, 0)\n",
        "        blood_vessel = blood_vessel == 2\n",
        "        blood_vessel = blood_vessel * 1\n",
        "\n",
        "        blood_vessel = blood_vessel.cpu().numpy()\n",
        "        all_encode = {}\n",
        "        for i in range(blood_vessel.shape[0]):\n",
        "            list_encode = []\n",
        "            sliceImage = blood_vessel[i,:,:]\n",
        "            binarized = sliceImage > 0\n",
        "            coded_len = self.encode_binary_mask(binarized)\n",
        "            list_encode.append(coded_len)\n",
        "            all_encode[idx[i]] = list_encode\n",
        "        return all_encode\n",
        "\n",
        "    def get_test_transforms(self):\n",
        "        \"\"\"\n",
        "        Get the test data augmentation transforms.\n",
        "\n",
        "        Returns:\n",
        "            A.Compose: Augmentation transforms.\n",
        "        \"\"\"\n",
        "        return A.Compose(CFG.test_aug)\n",
        "\n",
        "    def test_dataloader(self, image_folder):\n",
        "        \"\"\"\n",
        "        Get the dataloaders for testing.\n",
        "\n",
        "        Args:\n",
        "            image_folder: Folder containing test images.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: Test dataloader.\n",
        "        \"\"\"\n",
        "        tl, td, vl, vd = get_dataloader(shuffle_validation=True)\n",
        "        return vl, vd\n",
        "\n",
        "    def evaluate(self, model, test_dataloader, weights):\n",
        "        \"\"\"\n",
        "        Evaluate the model on the test dataset.\n",
        "\n",
        "        Args:\n",
        "            model: Model to evaluate.\n",
        "            test_dataloader: Dataloader for the test dataset.\n",
        "            weights: List of model weights for TTA.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: Evaluation results.\n",
        "        \"\"\"\n",
        "        ids = []\n",
        "        heights = []\n",
        "        widths = []\n",
        "        prediction_strings = []\n",
        "        sample = None\n",
        "        with torch.no_grad():\n",
        "            bar = tqdm(enumerate(test_dataloader), total=len(test_dataloader))\n",
        "\n",
        "            for step, (idn, images, x, y) in bar:\n",
        "                images = images.to(CFG.device)\n",
        "                images = torch.unsqueeze(images, 0)\n",
        "                ls = []\n",
        "                for weight in weights:\n",
        "                    model.load_state_dict(torch.load(weight))\n",
        "                    pred = TTA(images, model)\n",
        "                    ls.append(pred)\n",
        "                pred = torch.max(torch.stack(ls, dim=0), 0).values\n",
        "                _, _, h, w = images.shape\n",
        "                pred = F.interpolate(pred, size=[h, w], mode='bilinear', align_corners=False)\n",
        "                pred = pred[:, :, x:x+512, y:y+512]\n",
        "                pred_scored = torch.softmax(pred, 1)\n",
        "                if sample is None:\n",
        "                    sample = pred\n",
        "                pred_string = ''\n",
        "                pred = (pred_scored > 0.4).float().cpu().numpy()\n",
        "                pred_scored = pred_scored.cpu().numpy()\n",
        "                for m in range(len(pred)):\n",
        "                    kernel = np.ones(shape=(3, 3), dtype=np.uint8)\n",
        "                    x = ((pred[m][2] > 0.6) * 255.0)\n",
        "                    binary_mask = x\n",
        "                    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask.astype(np.uint8))\n",
        "                    for i in range(1, num_labels):\n",
        "                        mask_i = np.zeros_like(binary_mask)\n",
        "                        mask_i[labels == i] = 1\n",
        "                        mask = mask_i[:, :, np.newaxis].astype(np.bool)\n",
        "                        x_min, y_min, x_max, y_max = get_bounding_box(mask[:,:,0])\n",
        "                        if (x_min < 10) or (x_max > 500) or (y_min < 10) or (y_max > 500):\n",
        "                            mask_pred = mask_i * pred_scored[0, 2, :, :]\n",
        "                            score = np.sum(mask_pred) / np.sum(mask_i)\n",
        "                            encoded = self.encode_binary_mask(cv2.dilate(mask * 255.0, kernel, 3) > 0)\n",
        "                            if i == 0:\n",
        "                                pred_string += f\"0 {score} {encoded.decode('utf-8')}\"\n",
        "                            else:\n",
        "                                pred_string += f\" 0 {score} {encoded.decode('utf-8')}\"\n",
        "                b, c, h, w = images.shape\n",
        "                ids.append(idn)\n",
        "                heights.append(h)\n",
        "                widths.append(w)\n",
        "                prediction_strings.append(pred_string)\n",
        "        return ids, heights, widths, prediction_strings, sample, binary_mask\n",
        "\n",
        "# Create an instance of the Test class\n",
        "test = Test()\n",
        "\n",
        "# Call the evaluate method to perform inference and obtain results\n",
        "ids, heights, widths, prediction_strings, sample, binary_mask = test.evaluate(\n",
        "    model.float(),\n",
        "    dataset,\n",
        "    weights=[\"/kaggle/input/weights-1560/1/best_cross_entropy.pth\",\n",
        "             \"/kaggle/input/weights-1560/2/best_cross_entropy.pth\"] + glob.glob(\"/kaggle/input/weights-1560/*/\n",
        "             ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2Iyw3Uwho6l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}